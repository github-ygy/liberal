什么是多线程中的上下文切换？
    多线程会共同使用一组计算机上的CPU，而线程数大于给程序分配的CPU数量时，
    为了让各个线程都有执行的机会，就需要轮转使用CPU。
    不同的线程切换使用CPU发生的切换数据等就是上下文切换

j

死锁是怎样产生的
    所谓死锁，是指多个进程在运行过程中因争夺资源而造成的一种僵局，
    当进程处于这种僵持状态时，若无外力作用，它们都将无法再向前推进。
    死锁产生的四个条件（有一个条件不成立，则不会产生死锁）
    a.互斥条件：一个资源一次只能被一个进程使用
    b.请求与保持条件：一个进程因请求资源而阻塞时，对已获得资源保持不放
    c.不剥夺条件：进程获得的资源，在未完全使用完之前，不能强行剥夺
    d.循环等待条件：若干进程之间形成一种头尾相接的环形等待资源关系

比较下volatile和synchronized的异同点
    volatile可见性:
    每个线程都有一个自己的本地内存空间,线程执行时，先把变量从主内存读取到线程自己的本地内存空间，
    然后再对该变量进行操作,对该变量操作完后，在某个时间再把变量刷新回主内存,使用volatile关键字
    可以强迫线程从主内存读取数据
    volatile变量规则：对一个volatile域的写，happens- before 于任意后续对这个volatile域的读
    比较:
    volatile用于多线程环境下的单次操作(单次读或者单次写)。
    ①volatile轻量级，只能修饰变量。synchronized重量级，还可修饰方法
    ②volatile只能保证数据的可见性，不能用来同步，因为多个线程并发访问volatile修饰的变量不会阻塞。
    synchronized不仅保证可见性，而且还保证原子性，因为，只有获得了锁的线程才能进入临界区，从而保证临界区中的所有语句都全部执行。
    多个线程争抢synchronized锁对象时，会出现阻塞。

threadLocal的应用与实现原理
    threadlocal是将ThreadLocalMap放入Thread中管理，每个ThreadLocal是其中的key，这样
    threadlocal将需要保存的资源通过自身隔离在不同线程中，实现了资源设置与获取安全，同时由于map结构是放在
    thread中，所以thread结束gc回收时，同时也回收了存储的资源数据，但是如果使用线程池时，如果资源不使用后，
    不主动释放就会造成资源泄漏，所以ThreadLocalMap中的key的引用是弱引用，这样，当threadlocal被gc回收时，
    如果线程中的ThreadLocalMap有被其它Threadlocal调用，则线程中的资源也会被回收。否则就会有内存泄漏的问题。

如何将threadlocal在线程池中传递？
    TransmittableThreadLocal进行池化复用的线程池传递。
    用ttlrunnable增强runnable，构造runnable时，将静态容器hold(InheritableThreadLocal<Map<TransmittableThreadLocal<?>, ?)
    中存储的TransmittableThreadLocal keys备份copy传递后传递到当前线程，
    当runnable执行完成后，恢复copy的数据。(holder copy后的添加的数据需要在当前线程中清除)
    这样保证该子线程的数据都是parent所传递的数据

线程池如何实现的？具体参数的意义？
    流程:
        a.判断线程池里的核心线程是否都在执行任务，
            如果不是（核心线程空闲或者还有核心线程没有被创建）则创建一个新的工作线程来执行任务。
            如果核心线程都在执行任务，则进入下个流程。
        b.线程池判断工作队列是否已满，
            如果工作队列没有满，则将新提交的任务存储在这个工作队列里。
            如果工作队列满了，则进入下个流程。
        c.判断线程池里的线程是否都处于工作状态，
            如果没有，则创建一个新的工作线程来执行任务。
            如果已经满了，则交给饱和策略来处理这个任务。
    线程封装对象worker是Thread子类，while循环从队列中获取数据
    corePoolSize - 池中所保存的线程数，包括空闲线程。
    maximumPoolSize-池中允许的最大线程数。
    workQueue - 执行前用于保持任务的队列

线程池的增长策略与拒绝策略？
    增长策略
        workQueue - 执行前用于保持任务的队列
            a.直接提交，队里的数据只有被取走才能新增即数据直接提交线程处理(SynchronousQueue)
            b.无界队列，即无限排队新任务(LinkedBlockingQueue,PriorityBlockingQuene：具有优先级的无界阻塞队列)
            c.有界队列，超过容量限制需要提交线程处理(ArrayBlockingQueue)
        当正在执行任务数少于corePoolSize时，会直接新建线程执行任务，
        当正在执行任务数大于等于corePoolSize，之后的任务会进入workqueue排队
        当workqueue排队数已满时，如果正在执行的数据没有达到maximumPoolSize则新建线程执行
    拒绝策略
        当workqueue已满，且当前任务执行数量达到了maximumPoolSize则执行拒绝策略
        a.CallerRunsPolicy 直接使用当前线程直接执行
        b.AbortPolicy 直接抛出异常
        c.DiscardPolicy 不做任何处理，直接抛弃
        d.DiscardOldestPolicy lru将工作队列的头部任务抛弃，此任务保留

CAS缺点：
    ABA问题：
        比如说一个线程one从内存位置V中取出A，这时候另一个线程two也从内存中取出A，
        并且two进行了一些操作变成了B，然后two又将V位置的数据变成A，
        这时候线程one进行CAS操作发现内存中仍然是A，然后one操作成功。
        尽管线程one的CAS操作成功，但可能存在潜藏的问题。
    循环时间长开销大：
        对于资源竞争严重（线程冲突严重）的情况，CAS自旋的概率会比较大，
        从而浪费更多的CPU资源，效率低于synchronized。
    只能保证一个共享变量的原子操作：
        当对一个共享变量执行操作时，我们可以使用循环CAS的方式来保证原子操作，
        但是对多个共享变量操作时，循环CAS就无法保证操作的原子性，这个时候就可以用锁


CopyOnWriteArrayList的实现原理和思想?
    读写分离，读和写分开
    最终一致性
    使用另外开辟空间的思路，来解决并发冲突

fork join是什么？



CLH,MCS锁原理







